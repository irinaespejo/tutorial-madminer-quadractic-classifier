{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner particle physics tutorial\n",
    "\n",
    "# Part A5: Testing the new likelihood interface\n",
    "\n",
    "Johann Brehmer, Felix Kling, Irina Espejo, and Kyle Cranmer 2018-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10 madminer             INFO    \n",
      "16:10 madminer             INFO    ------------------------------------------------------------------------\n",
      "16:10 madminer             INFO    |                                                                      |\n",
      "16:10 madminer             INFO    |  MadMiner v0.7.1                                                     |\n",
      "16:10 madminer             INFO    |                                                                      |\n",
      "16:10 madminer             INFO    |         Johann Brehmer, Felix Kling, Irina Espejo, and Kyle Cranmer  |\n",
      "16:10 madminer             INFO    |                                                                      |\n",
      "16:10 madminer             INFO    ------------------------------------------------------------------------\n",
      "16:10 madminer             INFO    \n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import chi2, poisson\n",
    "\n",
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\"\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)\n",
    "\n",
    "from madminer import sampling\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.ml import ParameterizedRatioEstimator\n",
    "from madminer.likelihood import NeuralLikelihood, fix_params\n",
    "from madminer.utils.various import less_logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = SampleAugmenter(\"data/lhe_data_systematics.h5\")\n",
    "\n",
    "_ = sampler.sample_train_ratio(\n",
    "    theta0=sampling.random_morphing_points(1000, [('gaussian', 0., 15.), ('gaussian', 0., 15.)]),\n",
    "    theta1=sampling.benchmark('sm'),\n",
    "    nu0=sampling.iid_nuisance_parameters(),\n",
    "    nu1=sampling.nominal_nuisance_parameters(),\n",
    "    n_samples=10000,\n",
    "    folder='./data/samples',\n",
    "    filename='train_ratio_systematics',\n",
    "    sample_only_from_closest_benchmark=True,\n",
    "    return_individual_n_effective=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:29 madminer.ml          INFO    Starting training\n",
      "16:29 madminer.ml          INFO      Method:                 alices\n",
      "16:29 madminer.ml          INFO      alpha:                  1.0\n",
      "16:29 madminer.ml          INFO      Batch size:             128\n",
      "16:29 madminer.ml          INFO      Optimizer:              amsgrad\n",
      "16:29 madminer.ml          INFO      Epochs:                 20\n",
      "16:29 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:29 madminer.ml          INFO      Validation split:       0.25\n",
      "16:29 madminer.ml          INFO      Early stopping:         True\n",
      "16:29 madminer.ml          INFO      Scale inputs:           True\n",
      "16:29 madminer.ml          INFO      Scale parameters:       True\n",
      "16:29 madminer.ml          INFO      Shuffle labels          False\n",
      "16:29 madminer.ml          INFO      Samples:                all\n",
      "16:29 madminer.ml          INFO    Loading training data\n",
      "16:29 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio_systematics.npy into RAM\n",
      "16:29 madminer.utils.vario INFO      Loading data/samples/x_train_ratio_systematics.npy into RAM\n",
      "16:29 madminer.utils.vario INFO      Loading data/samples/y_train_ratio_systematics.npy into RAM\n",
      "16:29 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio_systematics.npy into RAM\n",
      "16:29 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio_systematics.npy into RAM\n",
      "16:29 madminer.ml          INFO    Found 10000 samples with 5 parameters and 3 observables\n",
      "16:29 madminer.ml          INFO    Setting up input rescaling\n",
      "16:29 madminer.ml          INFO    Rescaling parameters\n",
      "16:29 madminer.ml          INFO    Setting up parameter rescaling\n",
      "16:29 madminer.ml          INFO    Creating model\n",
      "16:29 madminer.ml          INFO    Training model\n",
      "16:29 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   1: train loss  1.05936 (improved_xe:  0.581, mse_score:  0.478)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.97686 (improved_xe:  0.514, mse_score:  0.463)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.97433 (improved_xe:  0.496, mse_score:  0.479)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.95033 (improved_xe:  0.485, mse_score:  0.465)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   3: train loss  0.95711 (improved_xe:  0.480, mse_score:  0.477)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.94937 (improved_xe:  0.475, mse_score:  0.474)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.94911 (improved_xe:  0.474, mse_score:  0.475)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.93972 (improved_xe:  0.473, mse_score:  0.467)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.94659 (improved_xe:  0.471, mse_score:  0.475)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.93238 (improved_xe:  0.468, mse_score:  0.465)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   6: train loss  0.94403 (improved_xe:  0.469, mse_score:  0.475)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.93239 (improved_xe:  0.466, mse_score:  0.466)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   7: train loss  0.94712 (improved_xe:  0.468, mse_score:  0.479)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92814 (improved_xe:  0.464, mse_score:  0.464)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   8: train loss  0.94681 (improved_xe:  0.467, mse_score:  0.480)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92856 (improved_xe:  0.466, mse_score:  0.463)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   9: train loss  0.93897 (improved_xe:  0.465, mse_score:  0.474)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.93849 (improved_xe:  0.464, mse_score:  0.474)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.93836 (improved_xe:  0.465, mse_score:  0.473)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92305 (improved_xe:  0.463, mse_score:  0.460)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  11: train loss  0.93678 (improved_xe:  0.464, mse_score:  0.473)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92495 (improved_xe:  0.461, mse_score:  0.464)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  12: train loss  0.93522 (improved_xe:  0.463, mse_score:  0.472)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92736 (improved_xe:  0.463, mse_score:  0.465)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  13: train loss  0.93568 (improved_xe:  0.463, mse_score:  0.472)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92985 (improved_xe:  0.462, mse_score:  0.468)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  14: train loss  0.93443 (improved_xe:  0.463, mse_score:  0.472)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92086 (improved_xe:  0.460, mse_score:  0.461)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.93265 (improved_xe:  0.462, mse_score:  0.471)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.91912 (improved_xe:  0.461, mse_score:  0.458)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  16: train loss  0.93313 (improved_xe:  0.462, mse_score:  0.472)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92374 (improved_xe:  0.461, mse_score:  0.463)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  17: train loss  0.93331 (improved_xe:  0.461, mse_score:  0.472)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92447 (improved_xe:  0.459, mse_score:  0.465)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  18: train loss  0.93516 (improved_xe:  0.461, mse_score:  0.474)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.91696 (improved_xe:  0.459, mse_score:  0.458)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  19: train loss  0.93109 (improved_xe:  0.461, mse_score:  0.470)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.92120 (improved_xe:  0.460, mse_score:  0.461)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.93000 (improved_xe:  0.460, mse_score:  0.470)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.91759 (improved_xe:  0.459, mse_score:  0.459)\n",
      "16:30 madminer.utils.ml.tr INFO    Early stopping after epoch 18, with loss  0.91696 compared to final loss  0.91759\n",
      "16:30 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "16:30 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:30 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "16:30 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "16:30 madminer.ml          INFO    Saving model to models/alices_systematics\n"
     ]
    }
   ],
   "source": [
    "estimator = ParameterizedRatioEstimator(\n",
    "    n_hidden=(100,),\n",
    "    activation=\"tanh\"\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='alices',\n",
    "    theta='data/samples/theta0_train_ratio_systematics.npy',\n",
    "    x='data/samples/x_train_ratio_systematics.npy',\n",
    "    y='data/samples/y_train_ratio_systematics.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio_systematics.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio_systematics.npy',\n",
    "    alpha=1.,\n",
    "    n_epochs=20,\n",
    ")\n",
    "\n",
    "estimator.save('models/alices_systematics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10 madminer.analysis.da INFO    Loading data from data/lhe_data_systematics.h5\n",
      "16:10 madminer.analysis.da INFO    Found 2 parameters\n",
      "16:10 madminer.analysis.da INFO    Found 3 nuisance parameters\n",
      "16:10 madminer.analysis.da INFO    Found 10 benchmarks, of which 6 physical\n",
      "16:10 madminer.analysis.da INFO    Found 3 observables\n",
      "16:10 madminer.analysis.da INFO    Found 57687 events\n",
      "16:10 madminer.analysis.da INFO      9858 signal events sampled from benchmark sm\n",
      "16:10 madminer.analysis.da INFO      47829 background events\n",
      "16:10 madminer.analysis.da INFO    Found morphing setup with 6 components\n",
      "16:10 madminer.analysis.da INFO    Found nuisance morphing setup\n",
      "16:10 madminer.ml.base     INFO    Loading model from models/alices_systematics\n"
     ]
    }
   ],
   "source": [
    "likelihood = NeuralLikelihood(\"data/lhe_data_systematics.h5\")\n",
    "\n",
    "nll = likelihood.create_expected_negative_log_likelihood(\n",
    "    model_file=\"models/alices_systematics\",\n",
    "    n_asimov=1000,\n",
    "    include_xsec=True,\n",
    "    theta_true=np.array([0.,0.]),\n",
    "    nu_true=np.array([0.,0.,0.]),\n",
    "    xsec_mode=\"interpol\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconstrained fit (overall best theta / nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.00695662, -0.12703062, -0.51023258,  6.11995267,  3.31061421])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = minimize(\n",
    "    nll,\n",
    "    x0 = np.array([0.1,-0.1,0.1,-0.1,0.1]),\n",
    "    method='L-BFGS-B',\n",
    ")\n",
    "best_fit = result.x\n",
    "best_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-check: explicit xsec calculation (no interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10 madminer.analysis.da INFO    Loading data from data/lhe_data_systematics.h5\n",
      "16:10 madminer.analysis.da INFO    Found 2 parameters\n",
      "16:10 madminer.analysis.da INFO    Found 3 nuisance parameters\n",
      "16:10 madminer.analysis.da INFO    Found 10 benchmarks, of which 6 physical\n",
      "16:10 madminer.analysis.da INFO    Found 3 observables\n",
      "16:10 madminer.analysis.da INFO    Found 57687 events\n",
      "16:10 madminer.analysis.da INFO      9858 signal events sampled from benchmark sm\n",
      "16:10 madminer.analysis.da INFO      47829 background events\n",
      "16:10 madminer.analysis.da INFO    Found morphing setup with 6 components\n",
      "16:10 madminer.analysis.da INFO    Found nuisance morphing setup\n",
      "16:10 madminer.ml.base     INFO    Loading model from models/alices_systematics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.00695662, -0.12703062, -0.51023258,  6.11995267,  3.31061421])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood2 = NeuralLikelihood(\"data/lhe_data_systematics.h5\")\n",
    "\n",
    "nll2 = likelihood2.create_expected_negative_log_likelihood(\n",
    "    model_file=\"models/alices_systematics\",\n",
    "    n_asimov=1000,\n",
    "    include_xsec=True,\n",
    "    theta_true=np.array([0.,0.]),\n",
    "    nu_true=np.array([0.,0.,0.]),\n",
    "    xsec_mode=\"blablubb\"\n",
    ")\n",
    "\n",
    "result2 = minimize(\n",
    "    nll2,\n",
    "    x0 = np.array([0.1,-0.1,0.1,-0.1,0.1]),\n",
    "    method='L-BFGS-B',\n",
    ")\n",
    "best_fit2 = result2.x\n",
    "best_fit2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing theta = [0., 0.], fitting nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1       , -0.09999823,  0.09999992])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constrained_nll = fix_params(nll, np.array([0., 0.]))\n",
    "\n",
    "result = minimize(\n",
    "    constrained_nll,\n",
    "    x0 = np.array([0.1,-0.1,0.1]),\n",
    "    method='L-BFGS-B',\n",
    ")\n",
    "constrained_best_fit = result.x\n",
    "constrained_best_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the profile log likelihood ratio (q) and asymptotic p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2342.23339313])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 2. * (constrained_nll(constrained_best_fit) - nll(best_fit))\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dof = 2\n",
    "p_value = chi2.sf(x=q, df=dof)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We've excluded the SM at super high significance :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (higgs_inference)",
   "language": "python",
   "name": "higgs_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
